{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation with late merge and hard parameters #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will be making a function to make a new directory with random augmented images that we will then feed to the dataloader to get both the original dataset and the new augmented image dataset for both classes in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_augmented_directory(old_path, newpath):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    classes = [i for i in os.listdir(old_path) if i != '.DS_Store']\n",
    "    for c in classes:\n",
    "        this_class_path = os.path.join(old_path, c)\n",
    "        all_folders_in_this_class = [i for i in os.listdir(this_class_path) if i!= '.DS_Store']\n",
    "        for folder in all_folders_in_this_class:\n",
    "            this_folder_path = os.path.join(this_class_path, folder)\n",
    "            images_in_this_folder = os.listdir(this_folder_path)\n",
    "            for image_f in images_in_this_folder:\n",
    "                image_path = os.path.join(this_folder_path, image_f)\n",
    "                img = Image.open(image_path)\n",
    "                augmented_image = transform(img)\n",
    "                augmented_image = transforms.ToPILImage()(augmented_image)\n",
    "                save_dir = os.path.join(newpath, c, folder)\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                save_path = os.path.join(save_dir, f'{image_f}')\n",
    "                augmented_image.save(save_path)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_augmented_directory(\"./dataset/\", \"./augmented_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1049\n",
      "2517\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir('./augmented_dataset')))\n",
    "print(len(os.listdir('./augmented_dataset/Snow')))\n",
    "print(len(os.listdir('./augmented_dataset/NotSnow')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_aug_dataloader(Dataset):\n",
    "    def __init__(self, main_data_dir ,secondary_data_dir, train):\n",
    "        self.data_dir = [main_data_dir, secondary_data_dir]\n",
    "        self.all_classes = ['NotSnow','Snow']\n",
    "        self.class_indexes = {cls: idx for idx, cls in enumerate(self.all_classes)}\n",
    "        self.dataset = self.dataset_maker(train)\n",
    "        self.length = len(self.dataset)\n",
    "    def dataset_maker(self, train):\n",
    "        data_list = []\n",
    "        for class_name in self.all_classes:\n",
    "            all_class_folders =os.path.join(self.data_dir[0], class_name)\n",
    "            for fold in os.listdir(all_class_folders):\n",
    "                if fold == \".DS_Store\":\n",
    "                    continue\n",
    "                sample_path_n_class = os.path.join(all_class_folders, fold)\n",
    "                sample = (sample_path_n_class, self.class_indexes[class_name])\n",
    "                data_list.append(sample)\n",
    "        for class_name in self.all_classes:\n",
    "            all_class_folders =os.path.join(self.data_dir[1], class_name)\n",
    "            for fold in os.listdir(all_class_folders):\n",
    "                if fold == \".DS_Store\":\n",
    "                    continue\n",
    "                sample_path_n_class = os.path.join(all_class_folders, fold)\n",
    "                sample = (sample_path_n_class, self.class_indexes[class_name])\n",
    "                data_list.append(sample)\n",
    "        random.shuffle(data_list)\n",
    "        if train:\n",
    "            return data_list[:int(0.8*len(data_list))]\n",
    "        else:\n",
    "            return data_list[int(0.8*len(data_list)):]\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample_path, sample_class = self.dataset[index]\n",
    "        pic1 = transforms.ToTensor()(Image.open(os.path.join(sample_path, '0.png')).convert('L')) # image 1 in tensor\n",
    "        pic2 = transforms.ToTensor()(Image.open(os.path.join(sample_path, '1.png')).convert('L')) # image 2 in tensor\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "        ])\n",
    "        pic1 = transform(pic1)\n",
    "        pic2 = transform(pic2)\n",
    "        return pic1 , pic2, sample_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug_training = data_aug_dataloader(\"./dataset/\", \"./augmented_dataset/\", True)\n",
    "data_aug_testing = data_aug_dataloader(\"./dataset/\", \"./augmented_dataset/\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 224])\n",
      "torch.Size([1, 224, 224])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "sample = data_aug_training[1311]\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)\n",
    "print(sample[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the new testing set has 1427 samples\n",
      "the new training set has 5705 samples\n"
     ]
    }
   ],
   "source": [
    "print(f'the new testing set has {len(data_aug_testing)} samples')\n",
    "print(f'the new training set has {len(data_aug_training)} samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class late_merge_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(late_merge_NN, self).__init__()\n",
    "        self.convolution1 = nn.Conv2d(1, 2, 1)\n",
    "        self.convolution2 = nn.Conv2d(2, 4, 1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=4, stride=5)\n",
    "        self.convolution12 = nn.Conv2d(1, 2, 1)\n",
    "        self.convolution22 = nn.Conv2d(2, 4, 1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4, stride=5)\n",
    "        self.full1 = nn.Linear(648, 2)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convolution1(x1)\n",
    "        x1 = self.pool(x1)\n",
    "        x1 = self.convolution2(x1)\n",
    "        x1 = self.pool(x1)\n",
    "        x1 = x1.view(-1, 324)\n",
    "        \n",
    "        x2 = self.convolution12(x2)\n",
    "        x2 = self.pool2(x2)\n",
    "        x2 = self.convolution22(x2)\n",
    "        x2 = self.pool2(x2)\n",
    "        x2 = x2.view(-1, 324)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.full1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate_7= 0.001\n",
    "epochs_7 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the neural net \n",
    "model7 = late_merge_NN()\n",
    "criterion7 = nn.CrossEntropyLoss()\n",
    "optimiser7 = optim.Adam(model7.parameters(), lr = learning_rate_7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of parameters in this model are 1330\n"
     ]
    }
   ],
   "source": [
    "# model params\n",
    "total_params = sum(p.numel() for p in model7.parameters())\n",
    "print(f'the total number of parameters in this model are {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is epoch number 0\n",
      "this is the 0 iteration\n",
      "loss: 48.515 accuracy: 75.200\n",
      "this is the 1000 iteration\n",
      "loss: 34.260 accuracy: 80.300\n",
      "this is the 2000 iteration\n",
      "loss: 27.527 accuracy: 83.367\n",
      "this is the 3000 iteration\n",
      "loss: 26.820 accuracy: 84.800\n",
      "this is the 4000 iteration\n",
      "loss: 26.843 accuracy: 85.680\n",
      "this is the 5000 iteration\n",
      "this is epoch number 1\n",
      "this is the 0 iteration\n",
      "loss: 20.115 accuracy: 92.700\n",
      "this is the 1000 iteration\n",
      "loss: 24.770 accuracy: 91.850\n",
      "this is the 2000 iteration\n",
      "loss: 22.845 accuracy: 92.100\n",
      "this is the 3000 iteration\n",
      "loss: 23.001 accuracy: 91.700\n",
      "this is the 4000 iteration\n",
      "loss: 23.420 accuracy: 91.660\n",
      "this is the 5000 iteration\n",
      "this is epoch number 2\n",
      "this is the 0 iteration\n",
      "loss: 18.056 accuracy: 93.900\n",
      "this is the 1000 iteration\n",
      "loss: 22.835 accuracy: 93.100\n",
      "this is the 2000 iteration\n",
      "loss: 21.528 accuracy: 93.167\n",
      "this is the 3000 iteration\n",
      "loss: 21.162 accuracy: 92.650\n",
      "this is the 4000 iteration\n",
      "loss: 21.708 accuracy: 92.620\n",
      "this is the 5000 iteration\n"
     ]
    }
   ],
   "source": [
    "model7.train()\n",
    "for epoch in range(epochs_7):\n",
    "    running_loss = 0 \n",
    "    predicted_list = []\n",
    "    labels_list = []\n",
    "    print(f'this is epoch number {epoch}')\n",
    "    for i, (input1,input2, label) in enumerate(data_aug_training):\n",
    "        optimiser7.zero_grad()\n",
    "        labels_list.append(label)\n",
    "        label = torch.tensor(label).view(-1)\n",
    "        if (i % 1000 == 0):\n",
    "            print(f'this is the {i} iteration')\n",
    "        outputs = model7(input1, input2)\n",
    "        loss = criterion7(outputs, label)\n",
    "        loss.backward()\n",
    "        optimiser7.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_list.append(predicted.item())\n",
    "        if i % 1000 == 999:   \n",
    "            print('loss: %.3f accuracy: %.3f' %(running_loss / 10, 100 * accuracy_score(labels_list, predicted_list)))\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model7.eval()\n",
    "loss = 0 \n",
    "predicted_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for i, (input1,input2, label) in enumerate(data_aug_testing):\n",
    "        label = torch.tensor(label)\n",
    "        label = torch.tensor(label).view(-1)\n",
    "        labels_list.append(label.item())\n",
    "        if (i % 1000 == 0):\n",
    "            print(f'this is the {i} iteration')\n",
    "        outputs = model7(input1, input2)\n",
    "        loss += criterion7(outputs, label).item()\n",
    "        predicted = outputs.argmax(dim = 1 , keepdim = True)\n",
    "        predicted_list.append(predicted.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model accuracy is 0.9264190609670637\n"
     ]
    }
   ],
   "source": [
    "print(f'the model accuracy is {accuracy_score(labels_list, predicted_list)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58585ea05f9f6f2b628a57adeb9d9d9b0d3552877a9b4072f333eacdc19945fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
