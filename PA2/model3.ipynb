{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early merge with hard parameters ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hard_dataloader(Dataset):\n",
    "    def __init__(self, data_dir , train):\n",
    "        self.data_dir = data_dir\n",
    "        self.all_classes = ['NotSnow','Snow']\n",
    "        self.class_indexes = {cls: idx for idx, cls in enumerate(self.all_classes)}\n",
    "        self.dataset = self.dataset_maker(train)\n",
    "        self.length = len(self.dataset)\n",
    "    def dataset_maker(self, train): # this will compile all the folder paths and the corresponding classes in a tuple to be stored in the class\n",
    "        data_list = []\n",
    "        for class_name in self.all_classes:\n",
    "            all_class_folders =os.path.join(self.data_dir, class_name)\n",
    "            for fold in os.listdir(all_class_folders):\n",
    "                if fold == \".DS_Store\":\n",
    "                    continue\n",
    "                sample_path_n_class = os.path.join(all_class_folders, fold)\n",
    "                sample = (sample_path_n_class, self.class_indexes[class_name])\n",
    "                data_list.append(sample)\n",
    "        random.shuffle(data_list)\n",
    "        if train:\n",
    "            return data_list[:int(0.8*len(data_list))]\n",
    "        else:\n",
    "            return data_list[int(0.8*len(data_list)):]\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample_path, sample_class = self.dataset[index]\n",
    "        pic1 = transforms.ToTensor()(Image.open(os.path.join(sample_path, '0.png')).convert('L')) # image 1 in tensor\n",
    "        pic2 = transforms.ToTensor()(Image.open(os.path.join(sample_path, '1.png')).convert('L')) # image 2 in tensor\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "        ])\n",
    "        pic1 = transform(pic1)\n",
    "        pic2 = transform(pic2)\n",
    "        return pic1.unsqueeze(0) , pic2.unsqueeze(0), sample_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_hard_training = hard_dataloader(\"./dataset/\", True)\n",
    "e_hard_testing = hard_dataloader(\"./dataset/\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = e_hard_training[321]\n",
    "print(sample2[0].shape)\n",
    "print(sample2[1].shape)\n",
    "print(sample2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class early_hard_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(early_hard_NN, self).__init__()\n",
    "        self.convolution2 = nn. Conv2d(1,2, kernel_size=3, stride=1)\n",
    "        self.convolution3 = nn. Conv2d(4, 2, kernel_size=2, stride=1)\n",
    "        self.convolution4 = nn. Conv2d(2, 2, kernel_size=3, stride=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=4, stride=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=5, stride=4)\n",
    "        self.fc1 = nn.Linear (1352, 2)\n",
    "        \n",
    "    def forward (self, x1,x2):\n",
    "        x2 = F.relu(self.convolution2(x2))\n",
    "        x2 = self.pool(x2)\n",
    "        x= torch.cat((x1, x2), dim=1)\n",
    "        x = F.relu(self.convolution3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.convolution4(x))\n",
    "        x = self.pool4(x)\n",
    "        x=torch.flatten (x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate3 = 0.001\n",
    "epochs3 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = early_hard_NN()\n",
    "criterion3 = nn.CrossEntropyLoss()\n",
    "optimiser3 = optim.Adam(model3.parameters(), lr=learning_rate3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of parameters in this model are 2798\n"
     ]
    }
   ],
   "source": [
    "# model params\n",
    "total_params = sum(p.numel() for p in model3.parameters())\n",
    "print(f'the total number of parameters in this model are {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.train()\n",
    "for epoch in range(epochs3):\n",
    "    running_loss = 0 \n",
    "    predicted_list = []\n",
    "    labels_list = []\n",
    "    print(f'this is epoch number {epoch}')\n",
    "    for i, (input1,input2, label) in enumerate(e_hard_training):\n",
    "        optimiser3.zero_grad()\n",
    "        labels_list.append(label)\n",
    "        label = torch.tensor(label).view(-1)\n",
    "        if (i % 1000 == 0):\n",
    "            print(f'this is the {i} iteration')\n",
    "        outputs = model3(input1, input2)\n",
    "        loss = criterion3(outputs, label)\n",
    "        loss.backward()\n",
    "        optimiser3.step()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_list.append(predicted.item())\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:   \n",
    "            print('loss: %.3f accuracy: %.3f' %( running_loss / 10, 100 * accuracy_score(labels_list, predicted_list)))\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.eval()\n",
    "loss = 0 \n",
    "predicted_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for i, (input1,input2, label) in enumerate(e_hard_testing):\n",
    "        label = torch.tensor(label)\n",
    "        label = torch.tensor(label).view(-1)\n",
    "        labels_list.append(label.item())\n",
    "        if (i % 1000 == 0):\n",
    "            print(f'this is the {i} iteration')\n",
    "        outputs = model3(input1, input2)\n",
    "        loss += criterion3(outputs, label).item()\n",
    "        predicted = outputs.argmax(dim = 1, keepdim = True)\n",
    "        predicted_list.append(predicted.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'the model accuracy is {accuracy_score(labels_list, predicted_list)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58585ea05f9f6f2b628a57adeb9d9d9b0d3552877a9b4072f333eacdc19945fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
