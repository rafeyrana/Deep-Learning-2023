{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification task using late merge with hard parameter sharing and inference resizing #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models are usually trained on a certain input shape which in our case is 1,224,224 but test samples are not usually of this size so they cannot be passed through a trained model and thus we have to reshape the test inputs for the model to process them we will be doing this using transforms to change the shape of the test data before feeding it to thr model in the evaluation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lmerge_dataloader(Dataset):\n",
    "    def __init__(self, path , train):\n",
    "        self.path = path\n",
    "        self.all_classes = ['NotSnow','Snow']\n",
    "        self.class_indexes = {cls: idx for idx, cls in enumerate(self.all_classes)}\n",
    "        self.dataset = self.data_lister(train)\n",
    "        self.length = len(self.dataset)\n",
    "        self.train = train\n",
    "    def data_lister(self, train): \n",
    "        data_list = []\n",
    "        for class_name in self.all_classes:\n",
    "            all_class_folders =os.path.join(self.path, class_name)\n",
    "            for fold in os.listdir(all_class_folders):\n",
    "                if fold == \".DS_Store\":\n",
    "                    continue\n",
    "                sample_path_n_class = os.path.join(all_class_folders, fold)\n",
    "                sample = (sample_path_n_class, self.class_indexes[class_name])\n",
    "                data_list.append(sample)\n",
    "        random.shuffle(data_list)\n",
    "        if train:\n",
    "            return data_list[:int(0.8*len(data_list))]\n",
    "        else:\n",
    "            return data_list[int(0.8*len(data_list)):]\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample_path, sample_class = self.dataset[index]\n",
    "      \n",
    "        pic1 = transforms.ToTensor()(Image.open(os.path.join(sample_path, '0.png')).convert('L')) # image 1 in tensor\n",
    "        pic2 = transforms.ToTensor()(Image.open(os.path.join(sample_path, '1.png')).convert('L')) # image 2 in tensor\n",
    "      \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "        ])\n",
    "        if self.train:\n",
    "            pic1 = transform(pic1)\n",
    "            pic2 = transform(pic2)\n",
    "        \n",
    "        return pic1.unsqueeze(0) , pic2.unsqueeze(0), sample_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_training = lmerge_dataloader('./dataset/', True)\n",
    "inference_testing = lmerge_dataloader('./dataset/', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224])\n",
      "torch.Size([1, 1, 224, 224])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_sample =inference_training[1311]\n",
    "print(train_sample[0].shape)\n",
    "print(train_sample[1].shape)\n",
    "print(train_sample[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 456, 640])\n",
      "torch.Size([1, 1, 456, 640])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "test_sample =inference_testing[131]\n",
    "print(test_sample[0].shape)\n",
    "print(test_sample[1].shape)\n",
    "print(test_sample[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class late_merge_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(late_merge_NN, self).__init__()\n",
    "        self.convolve1 = nn.Conv2d(1, 2, 1)\n",
    "        self.convolve2 = nn.Conv2d(2, 4, 1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=4, stride=5)\n",
    "        self.convolve12 = nn.Conv2d(1, 2, 1)\n",
    "        self.convolve22 = nn.Conv2d(2, 4, 1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4, stride=5)\n",
    "        self.full1 = nn.Linear(648, 2)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convolve1(x1)\n",
    "        x1 = self.pool(x1)\n",
    "        x1 = self.convolve2(x1)\n",
    "        x1 = self.pool(x1)\n",
    "        x1 = x1.view(-1, 324)\n",
    "        x2 = self.convolve12(x2)\n",
    "        x2 = self.pool2(x2)\n",
    "        x2 = self.convolve22(x2)\n",
    "        x2 = self.pool2(x2)\n",
    "        x2 = x2.view(-1, 324)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.full1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate_6= 0.001\n",
    "epochs_6 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the neural net with the stacking dataset\n",
    "model6 = late_merge_NN()\n",
    "criterion6 = nn.CrossEntropyLoss()\n",
    "optimiser2 = optim.Adam(model6.parameters(), lr = learning_rate_6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of parameters in this model are 1330\n"
     ]
    }
   ],
   "source": [
    "# model params\n",
    "total_params = sum(p.numel() for p in model6.parameters())\n",
    "print(f'the total number of parameters in this model are {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is epoch number 0\n",
      "this is the 0 iteration\n",
      "loss: 43.564 accuracy: 78.300\n",
      "this is the 1000 iteration\n",
      "loss: 27.102 accuracy: 83.050\n",
      "this is the 2000 iteration\n",
      "this is epoch number 1\n",
      "this is the 0 iteration\n",
      "loss: 25.418 accuracy: 90.200\n",
      "this is the 1000 iteration\n",
      "loss: 21.441 accuracy: 90.500\n",
      "this is the 2000 iteration\n",
      "this is epoch number 2\n",
      "this is the 0 iteration\n",
      "loss: 21.605 accuracy: 91.700\n",
      "this is the 1000 iteration\n",
      "loss: 18.224 accuracy: 92.300\n",
      "this is the 2000 iteration\n"
     ]
    }
   ],
   "source": [
    "model6.train()\n",
    "for epoch in range(epochs_6):\n",
    "    running_loss = 0 \n",
    "    predicted_list = []\n",
    "    labels_list = []\n",
    "    print(f'this is epoch number {epoch}')\n",
    "    for i, (input1,input2, label) in enumerate(inference_training):\n",
    "        optimiser2.zero_grad()\n",
    "        labels_list.append(label)\n",
    "        label = torch.tensor(label).view(-1)\n",
    "        if (i % 1000 == 0):\n",
    "            print(f'this is the {i} iteration')\n",
    "        outputs = model6(input1, input2)\n",
    "        loss = criterion6(outputs, label)\n",
    "        loss.backward()\n",
    "        optimiser2.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_list.append(predicted.item())\n",
    "        if i % 1000 == 999:   \n",
    "            print('loss: %.3f accuracy: %.3f' %(running_loss / 10, 100 * accuracy_score(labels_list, predicted_list)))\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l8/nqt3x5yn28z6g3mdkkcd_w940000gn/T/ipykernel_28973/366550427.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label).view(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the 0 iteration\n"
     ]
    }
   ],
   "source": [
    "model6.eval()\n",
    "# this transform will be used to change the input shapes in order to fit the model\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)), ])\n",
    "loss = 0 \n",
    "predicted_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for i, (input1,input2, label) in enumerate(inference_testing):\n",
    "        label = torch.tensor(label)\n",
    "        label = torch.tensor(label).view(-1)\n",
    "        labels_list.append(label.item())\n",
    "        # inference resizing implemented in the following two lines\n",
    "        input1 = transform(input1)\n",
    "        input2 = transform(input2)\n",
    "        if (i % 1000 == 0):\n",
    "            print(f'this is the {i} iteration')\n",
    "        outputs = model6(input1, input2)\n",
    "        loss += criterion6(outputs, label).item()\n",
    "        predicted = outputs.argmax(dim = 1 , keepdim = True)\n",
    "        predicted_list.append(predicted.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model accuracy is 0.9453781512605042\n"
     ]
    }
   ],
   "source": [
    "print(f'the model accuracy is {accuracy_score(labels_list, predicted_list)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58585ea05f9f6f2b628a57adeb9d9d9b0d3552877a9b4072f333eacdc19945fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
