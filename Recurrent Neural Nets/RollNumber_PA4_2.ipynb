{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CS437 PA4 Part 2 - Image Captioning with LSTMs [40 Marks]\n","\n","Roll Number:\n","\n","Name:"]},{"cell_type":"markdown","metadata":{},"source":["![A blueprint for an Image Captioning model](./assets/shikib.png)"]},{"cell_type":"markdown","metadata":{},"source":["Image captioning is a popular research area in deep learning that involves generating natural language descriptions of images. It is a challenging task that requires combining both computer vision and natural language processing (NLP) techniques to create a model capable of understanding the visual content of an image and generating a coherent and semantically meaningful sentence.\n","\n","One of the most successful approaches for image captioning involves the use of Long Short-Term Memory (LSTM) networks. LSTMs are a type of recurrent neural network (RNN) that can capture long-term dependencies and patterns in sequential data, such as text. By incorporating LSTMs into an image captioning model, it becomes possible to generate captions that are not only accurate but also contextually relevant and coherent."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T11:59:14.897705Z","iopub.status.busy":"2023-03-16T11:59:14.897421Z","iopub.status.idle":"2023-03-16T11:59:44.165798Z","shell.execute_reply":"2023-03-16T11:59:44.164507Z","shell.execute_reply.started":"2023-03-16T11:59:14.897678Z"},"trusted":true},"outputs":[],"source":["# Imports here\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import spacy\n","\n","import torch\n","from torch import nn\n","import torchvision\n","import torchvision.models as models\n","from torchvision.transforms import transforms\n","from torch import optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(torch.__version__)\n","print(torchvision.__version__)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Step 0: Load the data\n","\n","Setup the directory structure as follows (names may vary from the dataset shared):\n","```\n","RollNumber_PA4_2.ipynb\n","flickr8k\n","    images/\n","    captions.txt\n","```\n","\n","Preprocessing the data is notoriously difficult in NLP. Everything has been implemented for you in this section so you need not worry about the details regarding Tokenization, and setting up the Vocabulary and whatnot.\n","\n","Do read through the code to have a rough idea of what's going on behind the scenes: how we create a vocabulary for our text-based model based off tokens that occur frequently *enough* in our dataset, how we deal with numericalizing the strings, how we set up our inputs and labels etc."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T11:59:44.169242Z","iopub.status.busy":"2023-03-16T11:59:44.168396Z","iopub.status.idle":"2023-03-16T11:59:46.523683Z","shell.execute_reply":"2023-03-16T11:59:46.522560Z","shell.execute_reply.started":"2023-03-16T11:59:44.169202Z"},"trusted":true},"outputs":[],"source":["# Use Spacy for tokenization\n","spacy_eng = spacy.load('en_core_web_sm')\n","\n","# Create the Vocabulary class\n","class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        '''\n","        Initialize the Vocabulary class.\n","        The freq_threshold is the minimum frequency needed to include a token in the vocabulary, otherwise it is replaced by <UNK>.\n","        '''\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.stoi = {v: k for k, v in self.itos.items()}\n","        self.freq_threshold = freq_threshold # threshold for including a word in the vocabulary\n","\n","    def __len__(self):\n","        '''\n","        Returns the length of the vocabulary.\n","        '''\n","        return len(self.itos)\n","    \n","    def tokenizer_eng(self, text):\n","        '''\n","        Tokenizes English text from a string into a list of strings (tokens) and reverses it\n","        '''\n","        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n","    \n","    def build_vocabulary(self, sentence_list):\n","        '''\n","        Builds a vocabulary from a list of sentences.\n","        This involves tokenizing the sentences and adding the tokens to the vocabulary.\n","        Tokens will only be added to the vocab if their frequences exceed the predefined threshold.\n","        '''\n","\n","        # Dictionary to store the frequency of each word in the training set\n","        frequencies = {}\n","        idx = 4\n","\n","        # Loop through the sentences\n","        for sentence in sentence_list:\n","\n","            # Loop through the list of tokens in the sentence and update the frequencies dictionary\n","            for word in self.tokenizer_eng(sentence):\n","\n","                # Update word frequency\n","                if word not in frequencies:\n","                    frequencies[word] = 1\n","                else:\n","                    frequencies[word] += 1\n","\n","                # Add the word to the vocabulary if it exceeds the frequency threshold\n","                if frequencies[word] == self.freq_threshold:\n","                    self.stoi[word] = idx\n","                    self.itos[idx] = word\n","                    idx += 1\n","\n","    def numericalize(self, text):\n","        '''\n","        Converts a list of tokens into their corresponding indices.\n","        '''\n","\n","        # Tokenize the text\n","        tokenized_text = self.tokenizer_eng(text)\n","\n","        # Return a list of numericalized tokens\n","        return [\n","            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n","            for token in tokenized_text\n","        ]\n","    \n","\n","# Create the FlickrDataset class\n","class FlickrDataset(Dataset):\n","    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n","        '''\n","        Initialize the dataset.\n","        '''\n","        self.root_dir = root_dir\n","        self.captions_df = pd.read_csv(captions_file)\n","        self.transform = transform\n","\n","        # Get the images and the captions\n","        self.imgs, self.captions = self.captions_df['image'], self.captions_df['caption']\n","\n","        # Initialize the vocabulary and build it\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocabulary(self.captions.tolist())\n","\n","    def __len__(self):\n","        '''\n","        Returns the length of the dataset.\n","        '''\n","        return len(self.captions)\n","    \n","    def __getitem__(self, index):\n","        '''\n","        Returns an image and its corresponding caption.\n","        '''\n","        caption = self.captions[index]\n","        img_id = self.imgs[index]\n","\n","        # TODO: Read in the image using the image id (may have to fix this)\n","        img = Image.open(os.path.join(self.root_dir, \"images\", img_id)).convert(\"RGB\")\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        # Convert caption (string) to numericalized tokens\n","        # The caption would look something like: [SOS, 123, 456, 789, EOS]\n","        caption = [self.vocab.stoi[\"<SOS>\"]] + self.vocab.numericalize(caption) + [self.vocab.stoi[\"<EOS>\"]]\n","        caption = torch.tensor(caption)\n","\n","        # Input: Image, Target: Caption\n","        return img, caption\n","    \n","\n","# Create a Collater for the DataLoader\n","class Collate:\n","    def __init__(self, pad_idx):\n","        '''\n","        Initialize the collate function.\n","        This is necessary to provide to the DataLoader, so that it can pad the captions to some fixed length.\n","        '''\n","        self.pad_idx = pad_idx\n","\n","    def __call__(self, batch):\n","        '''\n","        Collate takes a list of samples returned by the dataset and creates a mini-batch.\n","        '''\n","        # Get the images\n","        imgs = [item[0].unsqueeze(0) for item in batch]\n","        imgs = torch.cat(imgs, dim=0)\n","\n","        # Get the captions and pad them\n","        targets = [item[1] for item in batch]\n","        targets = nn.utils.rnn.pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n","\n","        return imgs, targets\n","    \n","# Create the DataLoader\n","def get_loader(root_folder, annotation_file, transform, batch_size, shuffle=True):\n","    '''\n","    Returns a DataLoader for the dataset.\n","    '''\n","    \n","    # Create the dataset\n","    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n","\n","    # Get the token to pad the captions with (if they are shorter than some threshold)\n","    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","\n","    # Create the DataLoader\n","    loader = DataLoader(\n","        dataset=dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        pin_memory=True,\n","        collate_fn=Collate(pad_idx=pad_idx)\n","    )\n","\n","    return loader, dataset\n","\n","# Get the DataLoader\n","loader, dataset = get_loader(\n","    root_folder=\"./flickr8kimagescaptions/flickr8k\",\n","    annotation_file=\"./flickr8kimagescaptions/flickr8k/captions.txt\",\n","    transform=transforms.ToTensor(),\n","    batch_size=32\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T11:59:46.527036Z","iopub.status.busy":"2023-03-16T11:59:46.525374Z","iopub.status.idle":"2023-03-16T11:59:46.827641Z","shell.execute_reply":"2023-03-16T11:59:46.826802Z","shell.execute_reply.started":"2023-03-16T11:59:46.526996Z"},"trusted":true},"outputs":[],"source":["# Show an example\n","rand_idx = np.random.randint(0, len(dataset))\n","img, caption = dataset[rand_idx]\n","plt.imshow(img.permute(1,2,0))\n","plt.axis(False)\n","\n","\n","# Get the caption (the dataset provides it in numericalized form)\n","text_caption = []\n","for word_idx in caption:\n","    text_caption.append(dataset.vocab.itos[word_idx.item()])\n","text_caption = ' '.join(text_caption)    \n","plt.title(text_caption)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Step 1: Making a CNN Encoder [5 Marks] "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["By now you should have a good guess that the input to our grand model is a single image, on which we want to base a caption off.\n","\n","In the diagram at the start of the notebook, note how there's a CNN \"Encoder\" responsible for creating a rich feature representation of the image. The objective of the model you will implement in this section is to take in an image, pass it through some layers to generate a feature vector whose size we specify. It is very similar to what happens within an Autoencoder if any bells start ringing.\n","\n","Since we don't want to bother training a model from scratch, we will use a pretrained model here: specifically EfficientNetB2 (which performs well, and is relatively lightweight). We will bring in the pretrained weights and **freeze them** so that they are not changed during the training process (think about why this is efficient). We will replace the head of this model with our own Linear layer which condenses the final activation into the vector whose size we want."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T11:59:46.830533Z","iopub.status.busy":"2023-03-16T11:59:46.830206Z","iopub.status.idle":"2023-03-16T11:59:46.839714Z","shell.execute_reply":"2023-03-16T11:59:46.838614Z","shell.execute_reply.started":"2023-03-16T11:59:46.830501Z"},"trusted":true},"outputs":[],"source":["class CNNEncoder(nn.Module):\n","    def __init__(self, embed_size):\n","        super().__init__()\n","\n","        self.embed_size = embed_size\n","        \n","        # TODO: Get a pretrained model Feature Extractor (use EfficientNet-B2)\n","        model_weights = ???\n","        self.pretrained_encoder = ???\n","\n","        # TODO: Freeze the pretrained model\n","        ???\n","\n","        # TODO: Change the output layer to project the features to the desired embedding size\n","        self.pretrained_encoder.classifier = nn.Linear(???, ???)\n","\n","        # TODO: Make sure that the output layer is not frozen\n","        ???\n","\n","        # Other layers\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.5)\n","\n","\n","    def forward(self, images):\n","        '''\n","        Pass the images through the CNN encoder -> ReLU -> Dropout layers.\n","        '''\n","        return self.dropout(self.relu(self.pretrained_encoder(images)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T11:59:46.842067Z","iopub.status.busy":"2023-03-16T11:59:46.841386Z","iopub.status.idle":"2023-03-16T11:59:47.846181Z","shell.execute_reply":"2023-03-16T11:59:47.845064Z","shell.execute_reply.started":"2023-03-16T11:59:46.842028Z"},"trusted":true},"outputs":[],"source":["# Test whether this Encoder works\n","cnn_encoder = CNNEncoder(embed_size=256)\n","\n","# Pass the image from above into the model (note the output shape)\n","encoder_features = cnn_encoder(img.unsqueeze(0))\n","print(f\"CNN Encoder output shape: {encoder_features.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2: Making an RNN Decoder [10 Marks]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now that we have some nice features extracted from our model, we will hand this learned representation over to the LSTM decoder. This will take in that singular input, and generate a Sequence from it, which will be the Caption for that image.\n","\n","Note that in this model, we will actually provide the ground truth captions to the model during training. This is something called \"Teacher Forcing\": if at any point, the Decoder predicts a wrong word, the following predictions will also be messed up and hence will be penalized. In this event, we provide the ground truth captions so that the Decoder has something correct to work off at every time-step (think about this like avoiding \"Error Carried Forward\" in your exams).\n","\n","Also note the use of Embeddings here. We have to encode our captions while keeping its semantics (mostly) intact, which is why we take this approach as compared to One-Hot Encoding them."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T11:59:47.848632Z","iopub.status.busy":"2023-03-16T11:59:47.847638Z","iopub.status.idle":"2023-03-16T11:59:47.858228Z","shell.execute_reply":"2023-03-16T11:59:47.856807Z","shell.execute_reply.started":"2023-03-16T11:59:47.848589Z"},"trusted":true},"outputs":[],"source":["class RNNDecoder(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","        super().__init__()\n","\n","        # TODO: Embedding layer to convert word indices to word embeddings\n","        self.embedding = nn.Embedding(???, ???)\n","\n","        # TODO: LSTM layer to process the sequence of embeddings\n","        self.lstm = nn.LSTM(???, ???, ???)\n","\n","        # TODO: Linear layer to map the hidden state to the vocabulary\n","        self.linear = nn.Linear(???, ???)\n","        \n","        # Other layers\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, features, captions):\n","        '''\n","        The features are the output of the CNN encoder\n","        The captions are the input to the RNN decoder. During the Training phase, the captions are the ground truth captions.\n","        '''\n","\n","        # TODO: Generate the embeddings for the captions\n","        embeddings = ???\n","\n","        # TODO: Concatenate the features and the embeddings along the 0th dimension\n","        # Note: Add an extra dimension to the features to make it compatible with the embeddings\n","        embeddings = ???\n","\n","        # TODO: Process the embeddings using the LSTM layer\n","        lstm_out, _ = ???\n","\n","        # TODO: Map the LSTM output to the vocabulary\n","        out = self.linear(???)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T11:59:47.860421Z","iopub.status.busy":"2023-03-16T11:59:47.859984Z","iopub.status.idle":"2023-03-16T11:59:47.907903Z","shell.execute_reply":"2023-03-16T11:59:47.906855Z","shell.execute_reply.started":"2023-03-16T11:59:47.860384Z"},"trusted":true},"outputs":[],"source":["# Test whether this Decoder works\n","rnn_decoder = RNNDecoder(embed_size=256, hidden_size=256, vocab_size=len(dataset.vocab), num_layers=1)\n","print(f\"Vocab Size: {len(dataset.vocab)}\")\n","\n","# Pass the Encoder features through this Decoder\n","# Note how it takes a single feature vector and single caption (at a given time)\n","decoded_out = rnn_decoder(encoder_features.squeeze(), caption)\n","print(f\"Decoder Output shape: {decoded_out.shape}\")\n","\n","# Think about what the shape represents (along the lines of probabilities for vocab tokens)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Step 3: Bringing it together [20 Marks]\n","\n","Congrats! Now that you've made the Encoder and Decoder, we bring them together into one grand model.\n","\n","The meaty part here is the `caption_image` function that will take in a raw image, perform *all* the computations, and present the final generated caption (perfectly formatted and stringified). Since this has to generalize to new examples, we will not be able to take in the captions from the dataset as we did before. There is no notion of Teacher Forcing here.\n","\n","Note: You will only get full marks in this section (Step 3) if the final model shows improvement via the loss decreasing in Step 4 (next section). If you've written sensible code here but the model malfunctions, the max you will get is 10 Marks here (assuming the model outputs something)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T11:59:47.909972Z","iopub.status.busy":"2023-03-16T11:59:47.909546Z","iopub.status.idle":"2023-03-16T11:59:47.920311Z","shell.execute_reply":"2023-03-16T11:59:47.919262Z","shell.execute_reply.started":"2023-03-16T11:59:47.909933Z"},"trusted":true},"outputs":[],"source":["class CaptioningModel(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","        super().__init__()\n","\n","        # Encoder and Decoder\n","        self.encoder = CNNEncoder(embed_size)\n","        self.decoder = RNNDecoder(embed_size, hidden_size, vocab_size, num_layers)\n","\n","    def forward(self, images, captions):\n","        # Get the features from the encoder\n","        features = self.encoder(images)\n","\n","        # Pass the features and the captions to the decoder\n","        out = self.decoder(features, captions)\n","        \n","        return out\n","\n","    def caption_image(self, image, vocabulary, max_len=50):\n","\n","        # TODO: Create a list to store the caption as its generated\n","        result_caption = ???\n","\n","        with torch.no_grad():\n","\n","            # TODO: Get the image features from the encoder and add an extra dimension along the 0th axis\n","            x = ???\n","\n","            # Initialize the hidden state and cell state of the LSTM\n","            states = None\n","\n","            ## Generate the caption\n","            # Loop until the model hits max length\n","            for _ in range(max_len):\n","\n","                # TODO: Pass the features and the previous hidden state to the decoder\n","                hiddens, states = self.decoder.lstm(???, ???)\n","\n","                # TODO: Map the hidden state to the vocabulary\n","                # Squeeze the hidden state to make it compatible with the linear layer\n","                output = self.decoder.linear(???)\n","                \n","                # TODO: Take the argmax of the output along dimension 1 (the second dim) to get the index of the word with the highest probability\n","                predicted = ???\n","\n","                # Append the predicted word to the caption\n","                result_caption.append(predicted.item())\n","\n","                # TODO: Pass the predicted word to the embedding layer to get the embedding for the next time step\n","                x = self.decoder.embedding(???).unsqueeze(0)\n","\n","                # TODO: Break if the model predicts the <EOS> token\n","                # Hence, the caption will end when the model predicts the <EOS> token\n","                if vocabulary.itos[predicted.item()] == ???:\n","                    ???\n","\n","        # Convert the list of indices to a list of words\n","        return [vocabulary.itos[idx] for idx in result_caption]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now that the blueprint of the model is ready, we have to create the DataLoaders again. This is because the Encoder contains a pretrained model that had augmented the data in a specific way when it was trained before. For example, EfficientNetB2 expects images to be 288x288 in space, and normalized according to the ImageNet statistics. If you haven't used a different model, don't worry about any implementation details here."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T11:59:47.922655Z","iopub.status.busy":"2023-03-16T11:59:47.921896Z","iopub.status.idle":"2023-03-16T11:59:48.955448Z","shell.execute_reply":"2023-03-16T11:59:48.954393Z","shell.execute_reply.started":"2023-03-16T11:59:47.922614Z"},"trusted":true},"outputs":[],"source":["# Create the transforms for the images\n","# This will depend on the model you are using in the encoder\n","# If you used EffNetB2 as instructed, the transforms have been done for you :)\n","\n","effnet_tfms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize(288),\n","    transforms.CenterCrop(288),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# TODO: Create the DataLoader with these new transforms (may have to fix this)\n","loader, dataset = get_loader(\n","    root_folder=\"./flickr8kimagescaptions/flickr8k\",\n","    annotation_file=\"./flickr8kimagescaptions/flickr8k/captions.txt\",\n","    transform=effnet_tfms,\n","    batch_size=32\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T11:59:48.959295Z","iopub.status.busy":"2023-03-16T11:59:48.958819Z","iopub.status.idle":"2023-03-16T11:59:49.507088Z","shell.execute_reply":"2023-03-16T11:59:49.505839Z","shell.execute_reply.started":"2023-03-16T11:59:48.959253Z"},"trusted":true},"outputs":[],"source":["# Check if the model outputs something (hasn't been trained yet)\n","model = CaptioningModel(embed_size=256, hidden_size=256, vocab_size=len(dataset.vocab), num_layers=1)\n","raw_caption = model.caption_image(img.unsqueeze(0), dataset.vocab)\n","\n","print(f\"Model output: {' '.join(raw_caption)}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 4: Training the Model [5 Marks]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Hopefully you're in awe of the fact that you just created a model that captions an image. It's not that good right now though.\n","\n","Let's train it using the Dataset we bought in earlier.\n","\n","Most of the code has been provided here, you need only specify what the inputs and outputs should be for the overall model here (in the Forward pass and the Loss calculation)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T12:19:47.539691Z","iopub.status.busy":"2023-03-16T12:19:47.538981Z","iopub.status.idle":"2023-03-16T12:19:47.820191Z","shell.execute_reply":"2023-03-16T12:19:47.819118Z","shell.execute_reply.started":"2023-03-16T12:19:47.539649Z"},"trusted":true},"outputs":[],"source":["import time\n","\n","# Create dataset and dataloader (if you haven't run the above cells)\n","\n","# Hyperparameters\n","embed_size = 256\n","hidden_size = 256\n","vocab_size = len(dataset.vocab)\n","num_layers = 1\n","learning_rate = 3e-4\n","num_epochs = 5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Instantiate model\n","model = CaptioningModel(embed_size, hidden_size, vocab_size, num_layers).to(device)\n","\n","# Initialize loss and optimizer\n","criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","def train(\n","        model,\n","        train_loader=loader,\n","        criterion=criterion,\n","        optimizer=optimizer,\n","        num_epochs=num_epochs,\n","        device=device,        \n","        tfms=effnet_tfms\n","    ):\n","    \n","    model.train()\n","    epoch_losses = []\n","\n","\n","    for epoch in range(num_epochs):\n","\n","        epoch_loss = 0\n","        epoch_start = time.time()\n","\n","        for imgs, captions in train_loader:\n","\n","            # Cast the images and captions to the device\n","            imgs = imgs.to(device)\n","            captions = captions.to(device)\n","\n","            # TODO: Forward pass\n","            # Recall we pass both the images and the captions to the model\n","            # Note that we do not include the <EOS> token in the captions (hint: captions[:-1])\n","            outputs = model(???, ???)\n","\n","            # TODO: Calculate the loss\n","            # Recall that the outputs are of shape (seq_len, batch_size, vocab_size)\n","            # and the captions are of shape (batch_size, seq_len)\n","            # Hence, we need to reshape the outputs to (seq_len * batch_size, vocab_size) before passing to the loss_fn\n","            # Hint: Use the .reshape(???) method on the outputs, and the .reshape(-1) method on the captions\n","            loss = criterion(\n","                ???, ???\n","            )\n","\n","            # Zero the gradients\n","            optimizer.zero_grad()\n","            \n","            # Backward pass\n","            loss.backward(loss)\n","\n","            # Update the parameters\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()\n","            \n","        print(f\"Epoch {epoch+1} finished in {time.time()-epoch_start} seconds...\")\n","        \n","        print('-'*20)\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n","        print('-'*20)\n","            \n","        epoch_losses.append(epoch_loss/len(train_loader))\n","        \n","    return epoch_losses"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T12:19:48.398028Z","iopub.status.busy":"2023-03-16T12:19:48.396859Z","iopub.status.idle":"2023-03-16T12:57:09.710510Z","shell.execute_reply":"2023-03-16T12:57:09.709275Z","shell.execute_reply.started":"2023-03-16T12:19:48.397977Z"},"trusted":true},"outputs":[],"source":["losses = train(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T12:57:15.134222Z","iopub.status.busy":"2023-03-16T12:57:15.133854Z","iopub.status.idle":"2023-03-16T12:57:15.530701Z","shell.execute_reply":"2023-03-16T12:57:15.529735Z","shell.execute_reply.started":"2023-03-16T12:57:15.134191Z"},"trusted":true},"outputs":[],"source":["# Plot the loss curve\n","import seaborn as sns\n","plt.plot(losses)\n","\n","sns.despine()\n","plt.title(\"Plotting the Training Loss of the Image Captioning Model\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 5: Testing it out [0 Marks]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Here's hoping you must be seeing the loss go down in the above cell.\n","\n","Now try out your trained model on some images and see how it performs. Maybe try it on some of your own images and see if it makes any sense."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-16T13:02:55.933213Z","iopub.status.busy":"2023-03-16T13:02:55.932580Z","iopub.status.idle":"2023-03-16T13:02:56.118731Z","shell.execute_reply":"2023-03-16T13:02:56.117705Z","shell.execute_reply.started":"2023-03-16T13:02:55.933171Z"},"trusted":true},"outputs":[],"source":["# Using the model we just trained\n","model.eval()\n","\n","# Get a random image from the dataset\n","idx = np.random.randint(0, len(dataset))\n","image, correct_caption = dataset[idx]\n","\n","# Get the caption for the image\n","caption = model.caption_image(image.unsqueeze(0).to(device), dataset.vocab)\n","\n","plt.imshow(image.permute(1,2,0))\n","plt.axis(False)\n","\n","# Print the caption\n","correct_caption_decoded = []\n","for word_idx in correct_caption:\n","    try:\n","        correct_caption_decoded.append(dataset.vocab.itos[int(word_idx)])\n","    except:\n","        continue\n","print(f\"[CORRECT CAPTION]: {' '.join(correct_caption_decoded)}\")\n","print('-'*25)\n","print(f\"[OUTPUT]: {' '.join(caption)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Using the model we just trained\n","model.eval()\n","\n","# Get a random image from the dataset\n","idx = np.random.randint(0, len(dataset))\n","image, correct_caption = dataset[idx]\n","\n","# Get the caption for the image\n","caption = model.caption_image(image.unsqueeze(0), dataset.vocab)\n","\n","# Print the caption\n","print(f\"[CORRECT CAPTION]: {' '.join(correct_caption)}\")\n","print(f\"[OUTPUT]: {' '.join(caption)}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"bbe381b710e5d3541ca1e32a0f143d44d9fc319722adcf51c48d4250c2e9fef8"}}},"nbformat":4,"nbformat_minor":4}
