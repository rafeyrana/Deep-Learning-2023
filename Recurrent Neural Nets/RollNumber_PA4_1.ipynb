{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS437 PA4 Part 1 - Text Generation with RNNs [20 marks]\n",
    "\n",
    "Roll Number:\n",
    "\n",
    "Name:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text generation is a challenging task in natural language processing (NLP) that involves generating new text that is coherent and contextually relevant. Recurrent neural networks (RNNs) have shown to be effective for this task as they can learn long-term dependencies in sequential data and generate new text based on the context of previously generated text.\n",
    "\n",
    "In this assignment, we will explore the fundamentals of text generation with RNNs. We will begin by discussing the basics of NLP and RNNs, including their architecture and training procedures. By the end of this part, you will have created a model that will learn to (hopefully) produce jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Embeddings [5 Marks]\n",
    "\n",
    "One of the coolest things to play around with in NLP is Embeddings. You can find loads of articles online regarding where they are used, how they are trained, and why they are very useful. Put simply, they are fixed-size vector representations of tokens in natural language. This means you can represent words as vectors, sentences as vectors, even other entities like graphs as vectors!\n",
    "\n",
    "So what really makes them different from something like One-Hot vectors? They're still vectors, no? Let's ignore the fact you probably thought of it like that.\n",
    "\n",
    "What makes them special is that they have semantic meaning baked into them! This means you can model relationships between entities in text, which itself leads to a lot of fun applications. All modern architectures make use of Embeddings in some way.\n",
    "\n",
    "This part will allow you to explore what Embeddings are. We will load in pretrained Embeddings here and examine some of their properties. If you're interested, feel free look up the Word2Vec model: this is the model that was trained to give us the embeddings you will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pretrained word2vec model\n",
    "corpus = api.load('text8')\n",
    "model = Word2Vec(corpus)\n",
    "\n",
    "print(\"Done loading word2vec model!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded in the Embeddings, we can create an Embedding **layer** in PyTorch that will perform the processing step for us.\n",
    "\n",
    "Note in the following cell how there is a given **vocab size** and **embedding dimension** for the model: this is important to note because some sets of Embeddings may be defined for a large set of words (a large vocab), whereas older ones perhaps have a smaller set (a small vocab); the Embedding dimension essentially tells us how many *features* have been learned for a given word, that will allow us to perform further processing on top of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding layer using gensim\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(model.wv.vectors))\n",
    "\n",
    "# Get some information from the model\n",
    "print(f\"Vocab size: {len(model.wv.key_to_index)}\")\n",
    "\n",
    "print(f\"Some of the words in the vocabulary:\\n{list(model.wv.key_to_index.keys())[:10]}\")\n",
    "\n",
    "print(f\"Embedding dimension: {model.wv.vectors.shape[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for a demonstration, we instantiate two words, turn them into numbers (encoding them via their index in the vocab), and pass them through the Embedding layer. \n",
    "\n",
    "Note how the resultant Embeddings both have the same shape: 1 word, and 100 elements in the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take two words and get their embeddings\n",
    "word1 = \"king\"\n",
    "word2 = \"queen\"\n",
    "\n",
    "def word2vec(word):\n",
    "    return embedding_layer(torch.LongTensor([model.wv.key_to_index[word]]))\n",
    "\n",
    "king_embedding = word2vec(word1)\n",
    "queen_embedding = word2vec(word2)\n",
    "\n",
    "print(f\"Embedding Shape for '{word1}': {king_embedding.shape}\")\n",
    "print(f\"Embedding Shape for '{word2}': {queen_embedding.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have vectors whose scale is arbitrary, one nice way to measure how *similar* they are is with the Cosine Similarity measure.\n",
    "<center>\n",
    "    <img src=\"./assets/cosine-sim.png\">\n",
    "</center>\n",
    "\n",
    "We can apply this idea to our Embeddings. To see how \"similar\" two words are to the model, we can generate their Embeddings and take the Cosine Similarity of them. This will be a number between -1 and 1 (just like the range of the cosine function). When the number is close to 0, the words are not similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    '''\n",
    "    Computes the cosine similarity between two vectors\n",
    "    '''\n",
    "\n",
    "    vec1 = vec1.squeeze()\n",
    "    vec2 = vec2.squeeze()\n",
    "\n",
    "    # TODO: Compute the cosine similarity between the two vectors (using PyTorch)\n",
    "    return ???\n",
    "\n",
    "def compute_word_similarity(word1, word2):\n",
    "    '''\n",
    "    Takes in two words, computes their embeddings and returns the cosine similarity\n",
    "    '''\n",
    "\n",
    "    # TODO: Find the embeddings for the two words\n",
    "    word1_embedding = ???\n",
    "    word2_embedding = ???\n",
    "\n",
    "    # TODO: Compute the cosine similarity between the two embeddings\n",
    "    return ???\n",
    "\n",
    "# TODO: Define three words (one pair should be similar and one pair should be dissimilar) and compute their similarity\n",
    "word1 = ???\n",
    "word2 = ???\n",
    "word3 = ???\n",
    "print(f\"Similarity between '{word1}' and '{word2}': {compute_word_similarity(word1, word2)}\")\n",
    "print(f\"Similarity between '{word1}' and '{word3}': {compute_word_similarity(word1, word3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text with RNNs [15 Marks]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten a glimpse of what Embeddings are, we can move on to making our first text model.\n",
    "\n",
    "RNNs are capable of Seq2Seq modeling (mapping sequences to sequences). One form of this is generating text given an initial prompt. The model does this by:\n",
    "1. Numericalizing the input text (and converting them to Embeddings)\n",
    "2. Processing one set of tokens at a time\n",
    "3. Generating a probability distribution of what word is *most likely* to come next, given the previous words\n",
    "4. Predicting the word that maximizes this probability\n",
    "\n",
    "Since we'd like to be able to generate long texts, we will use an LSTM which is better at modeling longer sequences, as you likely have heard before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RNN model to generate text\n",
    "class LSTMGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # TODO: Create an Embedding layer given the vocab size and embedding dimension\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=???,\n",
    "            embedding_dim=???\n",
    "        )\n",
    "\n",
    "        # TODO: Create an LSTM layer given the embedding dimension, hidden dimension and number of layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=???,\n",
    "            hidden_size=???,\n",
    "            num_layers=???\n",
    "        )\n",
    "\n",
    "        # TODO: Create a fully connected layer given the hidden dimension and output dimension\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=???,\n",
    "            out_features=???\n",
    "        )\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        '''\n",
    "        Forward pass of the model\n",
    "        '''\n",
    "\n",
    "        # TODO: Get the embedding of the current input word (last output word)\n",
    "        embed = ???\n",
    "\n",
    "        # TODO: Get the output and the new hidden state by passing the lstm over embed and the previous state\n",
    "        output, state = ???\n",
    "\n",
    "        # TODO: Get the logits for the next word by passing the output of the LSTM through the fully connected layer\n",
    "        logits = ???\n",
    "\n",
    "        return logits, state\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initializes hidden state of the LSTM. We'll use this to pass an initial hidden state of zeros to the LSTM.\n",
    "        '''\n",
    "\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model was the simple part. The tricky part is processing our data for this task. \n",
    "\n",
    "<center>\n",
    "    <img src=\"./assets/LM1.jpg\">\n",
    "</center>\n",
    "\n",
    "We want to set up the task as follows. The model will take in a numericalized representation of a sequence of words. It's goal will be to generate the next word that should follow that sequence.\n",
    "\n",
    "One problem we will face is that creating tensors can be a bit problematic since the sentences are variable length. We will take a naive approach and simply flatten the whole dataset so it seems that we just have one *very long* string. This will make it much easier to generate pairs of sentences that are of the same length (that follow one another), and let us pack them into fixed-size tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for the jokes dataset\n",
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, path_to_jokes, sequence_length):\n",
    "        self.jokes_df = pd.read_csv(path_to_jokes)\n",
    "        self.jokes = self.jokes_df['Joke'].tolist()\n",
    "        self.words = self.jokes_df['Joke'].str.cat(sep=' ').split()\n",
    "        self.vocab = set([w for w in self.words])\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.vocab)}\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.vocab)}\n",
    "\n",
    "        self.word_indexes = [self.word_to_index[word] for word in self.words]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.jokes)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Return the item at the given index, making sure the sequence length is not exceeded\n",
    "        '''\n",
    "\n",
    "        return (\n",
    "            torch.tensor(self.word_indexes[index:index+self.sequence_length]),\n",
    "            torch.tensor(self.word_indexes[index+1:index+self.sequence_length+1]),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "jokes_dataset = JokesDataset('jokes.csv', sequence_length=10)\n",
    "jokes_dataloader = DataLoader(jokes_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're confused, take some time to print out a few examples from the Dataset object, check out the shapes and how they map to the words that we defined in the dictionaries.\n",
    "\n",
    "With that done, we can move on to training our model! The process is consistent with what we've done before. The only tricky part is formatting our model's predictions to resemble the ground truth (because of the LSTM), and dealing with the updated **Hidden** and **Cell** states. Otherwise, everything is the same since we've created an interface for our Dataset.\n",
    "\n",
    "Note here, we use the Cross Entropy Loss since we want the model to be able to classify what word is to come next. This is similar to basic Classification tasks since it is simply predicting a probability distribution of a number of classes (the vocab in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model,\n",
    "        dataloader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        epochs,\n",
    "        device,\n",
    "        sequence_length=10\n",
    "):\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # TODO: Initialize the hidden state\n",
    "        state_h, state_c = model.init_hidden(sequence_length)\n",
    "\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # TODO: Get the output from the model\n",
    "            # Hint: Look at the forward function of the model and notice that it returns two values\n",
    "            # Hint: The prev_state is a tuple of the Hidden state and the Cell state\n",
    "            y_pred, (state_h, state_c) = ???\n",
    "            y_pred = y_pred.transpose(1,2) # just some reshaping to make the loss function work\n",
    "\n",
    "            # TODO: Compute the loss\n",
    "            loss = ???\n",
    "\n",
    "            # Detach the hidden state from the graph to prevent backpropagation through the entire history\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            # TODO: Zero the gradients, perform a backward pass, and update the weights.\n",
    "            ???\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = LSTMGenerator(\n",
    "    vocab_size=len(jokes_dataset.vocab),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=100,\n",
    "    output_dim=len(jokes_dataset.vocab),\n",
    "    num_layers=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "train_losses = train(\n",
    "    model=model,\n",
    "    dataloader=jokes_dataloader,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.001),\n",
    "    epochs=30,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    sequence_length=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"CE Loss\")\n",
    "plt.title(\"Training Loss for LSTM Model\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're done with training the model, let's see how it performs when we give the prompt.\n",
    "\n",
    "Note that this can be slightly complicated since we don't have a conveniently formatted dataset, so we have to take care that the shapes of the Hidden and Cell states are correspond to the length we're working on.\n",
    "\n",
    "If your model doesn't generate jokes that make you laugh, there's a few things you can try:\n",
    "1. Open up your gradebook\n",
    "2. Tweak the sequence length your model sees\n",
    "3. Try out better Pretrained Embeddings (maybe ones that are more recent or have a higher dimensionality)\n",
    "4. Train for more epochs\n",
    "5. Use an LSTM with more layers\n",
    "\n",
    "Note that for the purposes for this assignment, don't worry about your model not generating something perfect, as long as it generates *something*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using the model\n",
    "def generate_text(model, dataset, seed_text, num_words=10):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    words = seed_text.split()\n",
    "    model.eval()\n",
    "\n",
    "    # TODO: Initialize the state of the LSTM corresponding to the length of the seed text\n",
    "    state_h, state_c = ???\n",
    "    \n",
    "    # Convert the seed text to a tensor\n",
    "    seed_text = torch.tensor([dataset.word_to_index[word] for word in seed_text.split()]).to(device)\n",
    "\n",
    "    # Initialize the output with the seed text\n",
    "    output = seed_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_words):\n",
    "            \n",
    "            x = [[dataset.word_to_index[word] for word in words[i:]]]\n",
    "            x = torch.tensor(x).to(device)\n",
    "\n",
    "            # TODO: Get the model output and updates states (same as before)\n",
    "            y_pred, (state_h, state_c) = ???\n",
    "\n",
    "            # Detach the hidden state from the graph to prevent backpropagation through the entire history\n",
    "            state_c = state_c.detach()\n",
    "            state_h = state_h.detach()\n",
    "\n",
    "            # Get the last predicted word\n",
    "            last_word_logits = y_pred[0][-1]\n",
    "            last_word_idx = torch.argmax(last_word_logits).unsqueeze(0)\n",
    "            words.append(dataset.index_to_word[last_word_idx.item()])            \n",
    "\n",
    "            # Append the last predicted word to the output\n",
    "            output = torch.cat((output, last_word_idx), dim=0)\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "generate_text(model, jokes_dataset, \"What did the science student say to the chicken?\", num_words=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbe381b710e5d3541ca1e32a0f143d44d9fc319722adcf51c48d4250c2e9fef8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
